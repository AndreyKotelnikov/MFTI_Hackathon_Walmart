{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59a0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Clear previous models/graphs\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def load_data(file_path='data/data_for_modeling.csv'):\n",
    "    df = pd.read_csv(file_path, parse_dates=['date'])\n",
    "    df.set_index('date', inplace=True)\n",
    "    return df[['unique_id', 'units']]\n",
    "\n",
    "\n",
    "def create_multitask_sequences(data, window, w_h, m_h, s_h):\n",
    "    \"\"\"\n",
    "    Generate sliding windows and future targets from a 1D array `data`.\n",
    "    \"\"\"\n",
    "    X_list, y_w, y_m, y_s = [], [], [], []\n",
    "    n = len(data)\n",
    "    for i in range(n - window - s_h + 1):\n",
    "        seq = data[i:i+window]\n",
    "        fut = data[i+window:i+window+s_h]\n",
    "        X_list.append(seq)\n",
    "        y_w.append(fut[:w_h])\n",
    "        y_m.append(fut[:m_h])\n",
    "        y_s.append(fut)\n",
    "    X = np.stack(X_list)[..., None]\n",
    "    return X, np.stack(y_w), np.stack(y_m), np.stack(y_s)\n",
    "\n",
    "\n",
    "def build_dataset_and_holdouts(df, window, season_horizon, month_horizon, week_horizon):\n",
    "    \"\"\"\n",
    "    For each unique_id series, split off its last horizons as holdouts,\n",
    "    but here we only return training sequences (no in-function validation).\n",
    "    \"\"\"\n",
    "    X_list, yw_list, ym_list, ys_list = [], [], [], []\n",
    "    for uid, group in df.groupby('unique_id'):\n",
    "        series = group['units'].sort_index().values\n",
    "        train_data = series\n",
    "        if len(train_data) < window + season_horizon:\n",
    "            continue\n",
    "        X, yw, ym, ys = create_multitask_sequences(\n",
    "            train_data, window, week_horizon, month_horizon, season_horizon\n",
    "        )\n",
    "        X_list.append(X)\n",
    "        yw_list.append(yw)\n",
    "        ym_list.append(ym)\n",
    "        ys_list.append(ys)\n",
    "    X_all = np.concatenate(X_list, axis=0)\n",
    "    y_w_all = np.concatenate(yw_list, axis=0)\n",
    "    y_m_all = np.concatenate(ym_list, axis=0)\n",
    "    y_s_all = np.concatenate(ys_list, axis=0)\n",
    "    return X_all, {'week': y_w_all, 'month': y_m_all, 'season': y_s_all}\n",
    "\n",
    "\n",
    "def build_multihead_model(window, week_horizon, month_horizon, season_horizon,\n",
    "                          lstm_units=50, dropout_rate=0.2):\n",
    "    inp = Input(shape=(window, 1), name='input')\n",
    "    x = LSTM(\n",
    "        lstm_units,\n",
    "        activation='tanh',\n",
    "        dropout=dropout_rate,\n",
    "        recurrent_dropout=dropout_rate,\n",
    "        name='lstm'\n",
    "    )(inp)\n",
    "    x = Dropout(dropout_rate, name='dropout')(x)\n",
    "    out_w = Dense(week_horizon, name='week')(x)\n",
    "    out_m = Dense(month_horizon, name='month')(x)\n",
    "    out_s = Dense(season_horizon, name='season')(x)\n",
    "    model = Model(inp, [out_w, out_m, out_s], name='multitask_lstm_dropout')\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss={'week': 'mse', 'month': 'mse', 'season': 'mse'},\n",
    "        loss_weights={'week': 1.0, 'month': 1.0, 'season': 1.0},\n",
    "        metrics={'week': [tf.keras.metrics.MeanSquaredError(name='mse')],\n",
    "                 'month': [tf.keras.metrics.MeanSquaredError(name='mse')],\n",
    "                 'season': [tf.keras.metrics.MeanSquaredError(name='mse')]}\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # Load data and parameters\n",
    "    df = load_data()\n",
    "    window, w_h, m_h, s_h = 30, 7, 30, 90\n",
    "\n",
    "    # Split unique_ids into train/validation groups\n",
    "    all_ids = df['unique_id'].unique()\n",
    "    train_ids, val_ids = train_test_split(all_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Build training set: all sequences from train_ids\n",
    "    X_train, y_train = build_dataset_and_holdouts(\n",
    "        df[df['unique_id'].isin(train_ids)],\n",
    "        window, s_h, m_h, w_h\n",
    "    )\n",
    "\n",
    "    # Build validation set: sequences from val_ids\n",
    "    X_val, y_val = build_dataset_and_holdouts(\n",
    "        df[df['unique_id'].isin(val_ids)],\n",
    "        window, s_h, m_h, w_h\n",
    "    )\n",
    "\n",
    "    # Build and train model with real hold-out\n",
    "    model = build_multihead_model(window, w_h, m_h, s_h)\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # Plot train/validation loss\n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.title('Total Loss'); plt.legend(); plt.show()\n",
    "\n",
    "    # Final evaluation on validation set\n",
    "    val_metrics = model.evaluate(X_val, y_val, verbose=0, return_dict=True)\n",
    "    print(\"\\nValidation Metrics:\", val_metrics)\n",
    "    # Compute per-series season MSE if desired\n",
    "    preds = model.predict(X_val)[2]\n",
    "    season_mse = mean_squared_error(y_val['season'], preds)\n",
    "    print(f\"Overall Season MSE: {season_mse:.3f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0c5f7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5472/5472 - 50s - 9ms/step - loss: 2994.7429 - month_loss: 862.8679 - season_loss: 858.7582 - val_loss: 740.4335 - val_month_loss: 234.1551 - val_season_loss: 205.1736 - val_week_loss: 200.9929 - week_loss: 851.1666\n",
      "Epoch 2/20\n",
      "5472/5472 - 46s - 8ms/step - loss: 2006.1699 - month_loss: 575.7144 - season_loss: 585.0865 - val_loss: 587.4216 - val_month_loss: 191.9827 - val_season_loss: 177.2074 - val_week_loss: 152.8823 - week_loss: 568.9123\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 106\u001b[0m\n\u001b[1;32m    104\u001b[0m model \u001b[38;5;241m=\u001b[39m build_multihead_model(window, w_h, m_h, s_h)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# training on train set only\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    107\u001b[0m     X_train, y_train,\n\u001b[1;32m    108\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m    109\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m    110\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val),\n\u001b[1;32m    111\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# plot training loss\u001b[39;00m\n\u001b[1;32m    116\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:377\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    376\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 377\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    378\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:223\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mget_value()\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution), iterator\n\u001b[1;32m    227\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/data/ops/optional_ops.py:194\u001b[0m, in \u001b[0;36m_OptionalImpl.get_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    186\u001b[0m   result \u001b[38;5;241m=\u001b[39m gen_optional_ops\u001b[38;5;241m.\u001b[39moptional_get_value(\n\u001b[1;32m    187\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor,\n\u001b[1;32m    188\u001b[0m       name\u001b[38;5;241m=\u001b[39mscope,\n\u001b[1;32m    189\u001b[0m       output_types\u001b[38;5;241m=\u001b[39mstructure\u001b[38;5;241m.\u001b[39mget_flat_tensor_types(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec),\n\u001b[1;32m    190\u001b[0m       output_shapes\u001b[38;5;241m=\u001b[39mstructure\u001b[38;5;241m.\u001b[39mget_flat_tensor_shapes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec),\n\u001b[1;32m    191\u001b[0m   )\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# NOTE: We do not colocate the deserialization of composite tensors\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# because not all ops are guaranteed to have non-GPU kernels.\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m structure\u001b[38;5;241m.\u001b[39mfrom_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec, result)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/data/util/structure.py:274\u001b[0m, in \u001b[0;36mfrom_tensor_list\u001b[0;34m(element_spec, tensor_list)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an element constructed from the given spec and tensor list.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    spec is not compatible with the tensor list.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-long-lambda\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _from_tensor_list_helper(\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m spec, value: spec\u001b[38;5;241m.\u001b[39m_from_tensor_list(value), element_spec,\n\u001b[1;32m    276\u001b[0m     tensor_list)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/data/util/structure.py:229\u001b[0m, in \u001b[0;36m_from_tensor_list_helper\u001b[0;34m(decode_fn, element_spec, tensor_list)\u001b[0m\n\u001b[1;32m    227\u001b[0m   flat_ret\u001b[38;5;241m.\u001b[39mappend(decode_fn(component_spec, value))\n\u001b[1;32m    228\u001b[0m   i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_flat_values\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(element_spec, flat_ret)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/data/util/nest.py:87\u001b[0m, in \u001b[0;36mpack_sequence_as\u001b[0;34m(structure, flat_sequence)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpack_sequence_as\u001b[39m(structure, flat_sequence):\n\u001b[1;32m     70\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a given flattened sequence packed into a nest.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m  If `structure` is a scalar, `flat_sequence` must be a single-element list;\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    ValueError: If nest and structure have different element counts.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m nest_util\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[1;32m     88\u001b[0m       nest_util\u001b[38;5;241m.\u001b[39mModality\u001b[38;5;241m.\u001b[39mDATA, structure, flat_sequence, expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     89\u001b[0m   )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/nest_util.py:859\u001b[0m, in \u001b[0;36mpack_sequence_as\u001b[0;34m(modality, structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m    856\u001b[0m       structure, flat_sequence, expand_composites, sequence_fn\n\u001b[1;32m    857\u001b[0m   )\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mDATA:\n\u001b[0;32m--> 859\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_data_pack_sequence_as(structure, flat_sequence)\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    862\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown modality used \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m for nested structure\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(modality)\n\u001b[1;32m    863\u001b[0m   )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/nest_util.py:964\u001b[0m, in \u001b[0;36m_tf_data_pack_sequence_as\u001b[0;34m(structure, flat_sequence)\u001b[0m\n\u001b[1;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    957\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not pack sequence. Argument `structure` had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    958\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(flat_structure)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements, but argument `flat_sequence` had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    959\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(flat_sequence)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements. Received structure: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    960\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstructure\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, flat_sequence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflat_sequence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    961\u001b[0m   )\n\u001b[1;32m    963\u001b[0m _, packed \u001b[38;5;241m=\u001b[39m _tf_data_packed_nest_with_indices(structure, flat_sequence, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sequence_like(structure, packed)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/nest_util.py:192\u001b[0m, in \u001b[0;36msequence_like\u001b[0;34m(instance, args)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the sequence `args` to the same type as `instance`.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m  `args` with the type of `instance`.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_mutable_mapping(instance):\n\u001b[1;32m    187\u001b[0m   \u001b[38;5;66;03m# Pack dictionaries in a deterministic order by sorting the keys.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m   \u001b[38;5;66;03m# Notice this means that we ignore the original order of `OrderedDict`\u001b[39;00m\n\u001b[1;32m    189\u001b[0m   \u001b[38;5;66;03m# instances. This is intentional, to avoid potential bugs caused by mixing\u001b[39;00m\n\u001b[1;32m    190\u001b[0m   \u001b[38;5;66;03m# ordered and plain dicts (e.g., flattening a dict but using a\u001b[39;00m\n\u001b[1;32m    191\u001b[0m   \u001b[38;5;66;03m# corresponding `OrderedDict` to pack it back).\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(_tf_core_sorted(instance), args))\n\u001b[1;32m    193\u001b[0m   instance_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(instance)\n\u001b[1;32m    194\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m instance_type \u001b[38;5;241m==\u001b[39m _collections\u001b[38;5;241m.\u001b[39mdefaultdict:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Clear previous models/graphs\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def load_data(file_path='data/data_for_modeling.csv'):\n",
    "    df = pd.read_csv(file_path, parse_dates=['date'])\n",
    "    df.set_index('date', inplace=True)\n",
    "    return df[['unique_id', 'units']]\n",
    "\n",
    "\n",
    "def build_multihead_model(window, week_horizon, month_horizon, season_horizon,\n",
    "                          lstm_units=50, dropout_rate=0.1):\n",
    "    inp = Input(shape=(window,1), name='input')\n",
    "    x = LSTM(lstm_units, activation='tanh',\n",
    "             dropout=dropout_rate, recurrent_dropout=dropout_rate,\n",
    "             name='lstm')(inp)\n",
    "    x = Dropout(dropout_rate, name='dropout')(x)\n",
    "    out_w = Dense(week_horizon, name='week')(x)\n",
    "    out_m = Dense(month_horizon, name='month')(x)\n",
    "    out_s = Dense(season_horizon, name='season')(x)\n",
    "    model = Model(inp, [out_w,out_m,out_s], name='multitask_lstm_dropout')\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss={'week':'mse','month':'mse','season':'mse'},\n",
    "        loss_weights={'week':2.0, 'month':1.0, 'season':0.5})\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_multitask_sequences(data, window, w_h, m_h, s_h):\n",
    "    \"\"\"\n",
    "    Generate sliding windows and future targets from a 1D array `data`.\n",
    "    \"\"\"\n",
    "    X, y_w, y_m, y_s = [], [], [], []\n",
    "    n = len(data)\n",
    "    for i in range(n - window - s_h + 1):\n",
    "        seq = data[i:i+window]\n",
    "        fut = data[i+window:i+window+s_h]\n",
    "        X.append(seq)\n",
    "        y_w.append(fut[:w_h])\n",
    "        y_m.append(fut[:m_h])\n",
    "        y_s.append(fut)\n",
    "    X = np.array(X)[...,None]\n",
    "    return X, np.array(y_w), np.array(y_m), np.array(y_s)\n",
    "\n",
    "\n",
    "def train_time_based(df, window, w_h, m_h, s_h):\n",
    "    \"\"\"\n",
    "    Splits each series by reserving the last `s_h` points for validation,\n",
    "    and the `window` points just before those for the validation input.\n",
    "    Returns concatenated training and validation sets.\n",
    "    \"\"\"\n",
    "    X_tr, yw_tr, ym_tr, ys_tr = [],[],[],[]\n",
    "    X_val, yw_val, ym_val, ys_val = [],[],[],[]\n",
    "\n",
    "    for uid, group in df.groupby('unique_id'):\n",
    "        series = group['units'].sort_index().values\n",
    "        if len(series) < window + s_h:\n",
    "            continue  # skip too-short series\n",
    "        # 1) validation target tail and its preceding window for input\n",
    "        val_tail = series[-s_h:]                    # true future\n",
    "        val_input = series[-s_h-window:-s_h]        # last window before tail\n",
    "        # 2) training data excludes both val_tail and val_input\n",
    "        train_data = series[:-s_h-window]\n",
    "        # 3) build training sequences\n",
    "        X_t, y_w_t, y_m_t, y_s_t = create_multitask_sequences(\n",
    "            train_data, window, w_h, m_h, s_h\n",
    "        )\n",
    "        X_tr.append(X_t); yw_tr.append(y_w_t); ym_tr.append(y_m_t); ys_tr.append(y_s_t)\n",
    "        # 4) pack validation single-sample per series\n",
    "        X_val.append(val_input.reshape(1,window,1))\n",
    "        yw_val.append(val_tail[:w_h].reshape(1,w_h))\n",
    "        ym_val.append(val_tail[:m_h].reshape(1,m_h))\n",
    "        ys_val.append(val_tail.reshape(1,s_h))\n",
    "\n",
    "    # concatenate all series\n",
    "    X_train = np.concatenate(X_tr,0)\n",
    "    y_w_train = np.concatenate(yw_tr,0)\n",
    "    y_m_train = np.concatenate(ym_tr,0)\n",
    "    y_s_train = np.concatenate(ys_tr,0)\n",
    "    X_valid = np.concatenate(X_val,0)\n",
    "    y_w_valid = np.concatenate(yw_val,0)\n",
    "    y_m_valid = np.concatenate(ym_val,0)\n",
    "    y_s_valid = np.concatenate(ys_val,0)\n",
    "\n",
    "    return (X_train, { 'week':y_w_train,'month':y_m_train,'season':y_s_train }),\\\n",
    "            (X_valid, { 'week':y_w_valid,'month':y_m_valid,'season':y_s_valid })\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    df = load_data()\n",
    "    window, w_h, m_h, s_h = 30, 7, 30, 90\n",
    "\n",
    "    (X_train, y_train), (X_val, y_val) = train_time_based(df, window, w_h, m_h, s_h)\n",
    "\n",
    "    model = build_multihead_model(window, w_h, m_h, s_h)\n",
    "    # training on train set only\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "\n",
    "    # plot training loss\n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    plt.title('Training Loss'); plt.legend(); plt.show()\n",
    "\n",
    "    # evaluate on held-out tails\n",
    "    print(\"Validation MSEs:\")\n",
    "    val_results = model.evaluate(X_val, y_val, verbose=0, return_dict=True)\n",
    "    print(val_results)\n",
    "    # or compute per-series MSE if needed\n",
    "    preds = model.predict(X_val)\n",
    "    season_mse = mean_squared_error(y_val['season'], preds[2])\n",
    "    print(f\"Overall Season MSE: {season_mse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d43fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
